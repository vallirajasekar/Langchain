{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_text_splitters import CharacterTextSplitter,RecursiveCharacterTextSplitter\n",
    "\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader=PyPDFLoader('Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 0}, page_content=' \\n                PROJECT ON PREDICTIVE MODELING          (LINEAR REGRESSION, LOGISTICS REGRESSION AND LDA)                                                        M VALLI RAJA SEKAR                             PGPDSBA.O.APR22.C                                                        \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content=\"                                             Linear Regression  You are a part of an investment firm and your work is to do research about these 759 firms. You are provided with the dataset containing the sales and other attributes of these 759 firms. Predict the sales of these firms on the bases of the details given in the dataset so as to help your company in investing consciously. Also, provide them with 5 attributes that are most important. The Main Aim here is to predict the sales using the 759 firms using the Attributes provided for the data and also to Find the 5 attribute that are most Important for prediction of the data.Here Nine Datasets are Provided   Data Dictionary for Firm_level_data: 1. sales: Sales (in millions of dollars). 2. capital: Net stock of property, plant, and equipment. 3. patents: Granted patents. 4. randd: R&D stock (in millions of dollars). 5. employment: Employment (in 1000s). 6. sp500: Membership of firms in the S&P 500 index. S&P is a stock market index that measures the stock performance of 500 large companies listed on stock exchanges in the United States 7. tobinq: Tobin's q (also known as q ratio and Kaldor's v) is the ratio between a physical asset's market value and its replacement value. 8. value: Stock market value. 9. institutions: Proportion of stock owned by institutions. \"),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 2}, page_content='1.1) Read the data and do exploratory data analysis. Describe the data briefly. (Check the null values, data types, shape, EDA). Perform Univariate and Bivariate Analysis. (8 marks) The shape of the Data contains of total 759 rows and 9 coloumns  The Header of the Data after dropping unnamed will be as follow    \\nThe Data contains one object as SP500 which is Categorical in Nature,Whereas the rest of the eight data types are either in Integer or Float in Nature.In which we can clearly Identify as there is missing Values present in tubing which need to be Identified and Treated   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 3}, page_content='On Identifying the null values, we had observed that the total Null Value Present in data is 21 which can be either treated with Mean or Median \\nBased on the Assumption, The Missing Value is treated with Mean on rest of the Data   \\nBefore and Treating the Missing Value with Mean, there is not Much changes in the Data. The Both had Attached above  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 4}, page_content='After Checking the Missing Value, the Next Procedure is to check whether Anomalies or Bad Data is present in the data  In Terms of Bad Data, there is no Bad data is present in the Data sent  For Anomalies,\\nThe Randd in above data contains lots of 0 present in the Data. Hence treating those with the value of Mean/ Median.We Might get better Predictions of the data instead of dropping those rows  \\nAfter Making above changes, we can see that only Means has been slightly changed Whereas the the Rest of the Dataâ€™s are present  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 5}, page_content='Data Visualization Univariate Analysis       Univariate data requires to analyze each variable separately. Uni means one, so in other words the data has only one variable.In Univariate Analysis, we can use Histogram and Box plot for Numerical type where as countplot for Categorical type  The Histogram and Box Plot Analysis of the eight numeric data are given below   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 6}, page_content='The Eight Numerical Data is presented in the form of Histogram and Boxplot and from the above observation we can clearly see that there is lots of Outlier present in the Data and also Histogram are not much Normally Distributed. Hence Treatment of Outliers and Standardisation is required for the above data.  The Analysis of one Categorical variable of SP500 are as follow   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 7}, page_content='In above data, Only 217 company are part of the SP500 whereas the rest are not part of SP500  Bivariate Analysis            In Bivariate Analysis, there are two variables wherein the analysis is related to cause and the relationship between the two variables.Here the Numeric and Numeric relation are established in the relationship using Pairplot/scatter plot whereas Categorical and Categorical are established with Countplot with Hue   \\n \\n \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 8}, page_content=' \\nOn above analysing the the numerical parameter of the data, we can clearly identify the data Dependent Variable of sales have some kind of correlation ship with all other Independent Variables present in the data ,while the relationship  can be better identified while finding Correlation ship and also while using the Heat Map for the data.  \\nOn above Analysing only one Independent Categorical Variable SP500 with only one Dependent Variable of sales.We can Identify that the sales is higher in SP500 member company than comparing with the the non member of SP500 The other most fascinating thing is that SP500 member company has higher capital compared to Non SP500 companies  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='Multivariate Analysis  Multivariate descriptive displays or plots are designed to reveal the relationship among several variables simulataneously.. As was the case when examining relationships among pairs of variables, there are several basic characteristics of the relationship among sets of variables that are of interest. Some of the Interesting Multivariate Analysis are pair plot,Heat Map, Facet etc Pairwise Analysis     A pairs plot allows us to see both distribution of single variables and relationships between two variable.The Pairwise plots are Mentioned below which determines the Overall relationship between the Dependent Variable and all other Independent variable. From this we Can identify microlevel and we can establish some kind of Relation   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 10}, page_content=' \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 11}, page_content='Heat map The heat map is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions. The variation in color may be by hue or intensity, giving obvious visual cues to the reader about how the phenomenon is clustered or varies over space.  The Data of correlation and Pictorial representation of Heat map are given below   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 12}, page_content='From the Heatmap we can identify the Correlationship of data in better way than establishing  the relationship in Univariate and Bivariate Method  Facetgrid    It is most useful when you have two discrete variables, and all combinations of the variables exist in the data.In below data, the higher is the sales, capital and tubing in SP500 compared to non SP500  \\n \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 13}, page_content='1.2) Impute null values if present? Do you think scaling is necessary in this case? (8 marks) Impute Null Values- The Data contains one object as SP500 which is Categorical in Nature,Whereas the rest of the eight data types are either in Integer or Float in Nature.In which we can clearly Identify as there is missing Values present in sp500 which need to be Identified and Treated  \\nOn Identifying the null values, we had observed that the total Null Value Present in data is 21 which can be either treated with Mean or Median \\nBased on the Assumption, The Missing Value of tubinq is  with Mean of the Data   Before and Treating the Missing Value with Mean, there is not Much changes in the Data. The Both had Attached above \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 14}, page_content='After Checking the Missing Value, the Next Procedure is to check whether Anomalies or Bad Data is present in the data  In Terms of Bad Data, there is no Bad data is present in the Data  \\nThe Randd in above data contains lots of 0 present in the Data. Hence  treating those with the value of Mean/ Median.We Might get better Predictions of the data instead of dropping those rows  \\u2029\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 15}, page_content='\\nAfter Making above changes, we can see that only Means has been slightly changed Whereas the the Rest of the Dataâ€™s are present are almost same  Hence We had Imputed the Null Values inform of Treating With Missing Values, Anamolies and Bad data with either treating Mean of variable  Do you think scaling is necessary in this case \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 16}, page_content='In above we can clearly see that there is difference in the Mean between their Attributes and also each attributes are measured in the different Parameters. Hence Scaling is required in the dataset  On applying the Z score for the data, the Data look  \\nBy doing Scaling , The Data got  normalised  and made mean as 0 and standard Deviation as 1. So by scaling all the data are in Normalised Manner  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 17}, page_content='1.3)Encode the data (having string values) for Modelling. Data Split: Split the data into test and train (30:70). Apply Linear regression. Performance Metrics: Check the performance of Predictions on Train and Test sets using R-square, RMSE \\nNow we can see that only SP 500  is the only category value  present in the Data. The SP 500 should be converted into either float or Integer by the process of Label Encoding Now info of the Data are as followed  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 18}, page_content='Hence now the whole data contains either float or Integer. Now we can Proceed the Data for Modelling Process  Now we need to split the data into test and Train in 30:70. Which Mean that the data contains 30 %  in test data and the rest 70% in Training data.Before Splitting the data into Test and Train, the first test is segregate Dependent Variable Sales from the Rest  of the Independent Variable. The Constant should be added to the Variable    \\nSplitting of Data are as follow \\nAs earlier the Test and Train data has  been splited  in the ratio of (30:70) and in the Random state of 5  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 19}, page_content='Applying the Linear regression in the Model using the OLS Method  On the Applying the linear regression using the OLS Method  The Model Summary looks like Below  \\nIn the Above Model R- Squared and Adj R-Squared Value is 0.928 and 0.927. But lots of attributes Present in the data has P-value >=0.5 hence most of Attributes become less Significant in this case. So the Multicollinearity using the VIF should be checked and select the last 5 best attributes  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 20}, page_content='\\nIn case of VIF for Multicollinearity greater than 4 or 5 can make the curse of Dimensionality. So the Value of VIF should be treated and need to be dropped and then  nessacary actions can be taken  The Below Scenario Can be explain the for best scenario of the data Modelling  Scenario 1  Here in scenario 1 our focus will be getting the 5 best attributes based on expert Opinion of the Data and deducting the nessacary based on our requirement with based on Expert   At first by dropping the Value, the data summary and VIF looks alike  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 21}, page_content='  \\nHere after dropping the values, the R squared value dropped to 91.3% and the VIF values is less than 5 but we can have some p-values greater than 0.5 So we can now retain institutions and drop the tobinq and patents on the basis of Importance of attributes  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 22}, page_content='The model looks like below, \\nNow comparing to other model, the R and adj R2 has not been changed much compared to other scenario and we can able to get the best attributes here  Paramas of  the Model are as below \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 23}, page_content='The equation of the model are as follow,  \\n Some Assumption on Linear Regression- 1).Non Linear on Residual      The Data below are Non linear in Nature \\n    \\n \\n \\n      \\n2).Normality    The data are normally Distributed  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 24}, page_content='3)Homoscedastic of Data    The data are Homoscedastic in nature   \\n4)Collinearity    Using VIF, we determined that there is no multicollinearity is present in the data  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 25}, page_content='RMSE of the Model    The RMSE of the Model for Training and testing are very low and we can deploy the Model   \\nMean Absolute Error   The MAE of the both training and Testing are also too low  \\nR squared and Adjusted R2 are also 91.3% and 91.2%  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 26}, page_content='The Prediction of the Data   price = 0.028299115503080438 + 0.4777681981583373 * ( -0.168736 ) +  0.14814498002360377 * ( -0.203250 ) +  0.45950021723081313 * ( -0.227855 ) +  0.011256618008760263 * ( -0.632747 ) +  0.00208124381198248 * ( -1.461405) Price= -0.19729159975652844 The Data has been predicted correctly using trained value on the Tested Value  \\nThe Data are scaled in Nature, So the Value is replicating the Negative Value  Hence by  Rescaling we can get the Original Values  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 27}, page_content='Scenario 2  In below  scenario, the RMSE and MSE are higher compared to Scenario 1  The R and Adj R2 reduced drastically to the 0.859 and 0.857. The Scenario can be Rejected We had dropped education  from the data instead of Patents.So education cannot be dropped at any scenario  \\n\\u2029\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 28}, page_content='Scenario 3  In below  scenario, the RMSE and MSE are higher compared to Scenario 1  The R and Adj R2 reduced drastically to the 0.859 and 0.857. The Scenario can be Rejected we had dropped all expect the capital,randd,sp500 and Institutions  \\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='So from the Over all basis, Scenario 1 better compared other scenarios because of the Higher R2 and adj R2 values in data and lesser error in RMSE  and MAE  So the Scenario 1 is best for the Predictions and the 5 attributes that contribute much for the data are as follow  \\nCapital ,randd, employment,SP 500 and Institutions are the top 5 attributes  2.4) Inference: Based on these predictions, what are the insights and recommendations? (6 marks) â€¢The capital such as Net Stock of Property,plant and Equipment which plays major in contributing the Sales to the Firm.The Major aspect as an Investment firm is to check the company with the Higher Capital which in turn make an higher Sales  â€¢The Second is most important aspect is to check the Employment of the Firm, Whenever the employment got Increased the company make better sales. As an investor is to check the employment in the firm of the Company and to make Better decision while investing in the firm â€¢The R&D plays the role of higher sales. So while Investing we should check how much the Firm is keen on Investing the R & D. This Parameter makes small edge over other Investment Firm â€¢The Institution  such Proportion of Stock owned by Institutions. The Institutions will make huge Investment compared to the retail Investor. So Being an Firm We need to check how much institution contributing for Stock price will make an added advantage while Investing â€¢In terms of SP 500, the member of Member of SP 500 has higher chance of better sales. So our most important aspect here is to check the SP500 company  â€¢Other Attributes such Patents,Dobinq, value etc plays a role but less than what is compared in above in the Model. So we can still concentrate those factor also \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content=\"Problem 2: Logistic Regression and Linear Discriminant Analysis You are hired by the Government to do an analysis of car crashes. You are provided details of car crashes, among which some people survived and some didn't. You have to help the government in predicting whether a person will survive or not on the basis of the information given in the data set so as to provide insights that will help the government to make stronger laws for car manufacturers to ensure safety measures. Also, find out the important factors on the basis of which you made your predictions. 1. dvcat: factor with levels (estimated impact speeds) 1-9km/h, 10-24, 25-39, 40-54, 55+ 2. weight: Observation weights, albeit of uncertain accuracy, designed to account for varying sampling probabilities. (The inverse probability weighting estimator can be used to demonstrate causality when the researcher cannot conduct a controlled experiment but has observed data to model) 3. Survived: factor with levels Survived or not_survived 4. airbag: a factor with levels none or airbag 5. seatbelt: a factor with levels none or belted 6. frontal: a numeric vector; 0 = non-frontal, 1=frontal impact 7. sex: a factor with levels f: Female or m: Male 8. ageOFocc: age of occupant in years 9. yearacc: year of accident 10. yearVeh: Year of model of vehicle; a numeric vector 11. abcat: Did one or more (driver or passenger) airbag(s) deploy? This factor has levels deploy, nodeploy and unavail 12. occRole: a factor with levels driver or pass: passenger 13. deploy: a numeric vector: 0 if an airbag was unavailable or did not deploy; 1 if one or more bags deployed. 14. injSeverity: a numeric vector; 0: none, 1: possible injury, 2: no incapacity, 3: incapacity, 4: killed; 5: unknown, 6: prior death 15. caseid: character, created by pasting together the population's sampling unit, the case number, and the vehicle number. Within each year, use this to uniquely identify the vehicle. \"),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 31}, page_content='2.1) Data Ingestion: Read the dataset. Do the descriptive statistics and do null value condition check, write an inference on it. Perform Univariate and Bivariate Analysis. Do exploratory data analysis. (8 marks) The head of the Data are as follow   The shape of the data after removing unnamed:0 as  (11217,15) \\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='From the Info of the Data we see clearly that  there is some missing Value in injseverity . The total object is 8,where as float and Integer is totally 7. So we need to check the Missing data, Anomalies and Bad value in the Data and it should be treated  \\nFor Treating Missing data, Anomalies and Bad value in the Data, info and describe of the should be Verified treating those Value  In above weight is 0 which is kind of Anomalies which should be treated at first, but we can see lot of weight ranging from 0 to 1 which is higher than that of 0 so I had decided not to treat those Anomalies unless and until if there is issue with my model  The Next step is to treat the  Missing Value present in the injseverity   \\nThe total Injseverity missing in Data is 77,This 77 should  be treated for the missing value Here the size of the model is 11217, so we can drop those 77 data which data become 10400 which is still holding good for prediction  There is no bad value present in the Data and total of two duplicated row and the row got dropped  Hence above the Null check condition is verified  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 33}, page_content='Data Visualisation  Univariate Analysis       Univariate data requires to analyze each variable separately. Uni means one, so in other words the data has only one variable.In Univariate Analysis, we can use Histogram and Box plot for Numerical type where as countplot for Categorical type  The Histogram and Box Plot Analysis of the eight numeric data are given below  \\n\\nThe Six Numerical Data is presented in the form of Histogram and Boxplot and from the above observation we can clearly see that there is lots of Outlier present in the Data and also Histogram are not much Normally Distributed. Hence Treatment of Outliers and Standardisation is required for the above data.  Categorical Variable  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 34}, page_content='  In Below are analyse of the Categorical in the data with the value counts. Hence the they are attached below  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 35}, page_content='\\n\\n\\u2029\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 36}, page_content='      \\n\\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 37}, page_content='\\n\\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 38}, page_content='\\nBivariate Analysis  In Bivariate Analysis, there are two variables wherein the analysis is related to cause and the relationship between the two variables.Here the Numeric and Numeric relation are established in the relationship using Pairplot/scatter plot whereas Categorical and Categorical are established with Countplot with Hue Scatter Plot \\nIn terms between the InjSeverity and Weight,Higher the Weight the injseverity is low  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 39}, page_content='  \\n  In between the â€˜yearaccâ€™ and â€˜Injseverityâ€™ ,The Injseverity got reduced when the when the year got  Increased which we can identify that the Introduction of many safety measure got reduced the Injseverity\\n\\n   Even  interms of weight also we can see that weight got Increased  as Year Increases \\n\\n            In term of ageOFocc and injseverity we can see that there is kind of no relationship  \\nComparing Independent Variable and Dependent Variable using Bi_variate Analysis \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 40}, page_content='When Injseverity is low, the People got Survived and when it is above 3 there is kind of some causality  \\nWhen speed is low, there is no causality but when it is above there is kind of Non Survival,When above 55+ there there is less 50% chance for Survival  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 41}, page_content='Whe with the Airbag, the Survival rate is Higher compared to Non Survival without  the Airbag  \\nInitially in year such as 1997, there  is non survival because the reason might be such as Technology, Awarness etc but later in the year on above 2000 there is Kind of Survival Existed but still there is causalities  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='\\nIf the Airbag is Unavailed or Nor Deployed then the chance of Causality is higher, When deployed the Survival Rate is higher compared to other scenarios   Multivariate Analysis  Multivariate descriptive displays or plots are designed to reveal the relationship among several variables simulataneously.. As was the case when examining relationships among pairs of variables, there are several basic characteristics of the relationship among sets of variables that are of interest. Some of the Interesting Multivariate Analysis are pair plot,Heat Map, Facet etc Pairwise Analysis     A pairs plot allows us to see both distribution of single variables and relationships between two variable.The Pairwise plots are Mentioned below which determines the Overall relationship between the Dependent Variable and all other Independent variable. From this we Can identify microlevel and we can establish some kind of Relation which is explained below     \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 43}, page_content='\\nFacetgrid- facet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 44}, page_content='Correlation in Data  The Correlation in the following data can be explained in the form of the Pictorial in Heat Map  \\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 45}, page_content='\\nCorrelation measures the relationship between two variables. We mentioned that a function has a purpose to predict a value. We can say also say that a function uses the relationship between two variables for prediction.In above Pictorical Representation will represent more Above about the Correlation  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 46}, page_content='Detecting the Outlier in the Data  Along With those Analysis one of important aspect is the Detecting the Outlier in the Data  We Can see in below that only weight has Outlier while rest of them donâ€™t have any kind of Outlier  Its will clearly  visible after doing the Scaling of the Data  \\n\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='We can Cap the Outlier only in Weight and Year Vech while rest are with the lower values.Hence there is no requirement of detecting outlier in other Attributes unless and until it is essential  2.2) Encode the data (having string values) for Modelling. Data Split: Split the data into train and test (70:30). Apply Logistic Regression and LDA (linear discriminant analysis). (8 marks) For Encoding the data  present in the variable we need to first Identify the total categorical variable present in the Data. In the below data we can see there are total of seven Category available where as the rest of the data present are in int or Float. The second is to identify how to encode those categorical variable into numerical, All the data mentioned below are ordinal, so we can encode those data point directly instead of using the one hot Encoder  \\n   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 48}, page_content='Before Converting the categorical into the Numerical Variable  \\n\\n     After Converting those Categorical into the      numerical Variable  \\nBefore Splitting the Data, the most essential thing is to Segregate the Independent and Dependent Variable  in the Data  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 49}, page_content='Splitting of data  Logistics Regression  In Logistic Regression, We had Segregated the X and y as X_train,X_test,y_train and y_test with the test size of 0.30 and the random state as 1  Linear Discriminant Analysis  In Linear Discriminant Analysis, We had Segregated the X and y as X_train_1,X_test_1,y_train_1 and y_test_1 with the test size of 0.30 and the random state as 1  Model Building on the Data  Logistics Regression  \\nIn Logistic Regression, We had Build the model using the solver,max_iter,verbose etc to Increase the Accuracy of the Model.We had trained both X and y variable in the Data  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 50}, page_content='In Above We had predicted for both y train_predict and y test_predict and also interns of Probability also  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 51}, page_content='Linear Discriminant Analysis  \\n \\nIn LDA, We had just  trained  the Data in both X and y variable in the Data and not made any complication in Building the LDA Model  \\nIn Above We had predicted for both y train_predict_1and y test_predict_1 and also interms of Probability also  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 52}, page_content='2.3) Performance Metrics: Check the performance of Predictions on Train and Test sets using Accuracy, Confusion Matrix, Plot ROC curve and get ROC_AUC score for each model. Compare both the models and write inferences, which model is best/optimized. (8 marks) MODEL  ACCURACY SCORE-  \\nIn Model  Accuracy Score,The Logistic Regression has fared better than LDA in both Training and Testing.  CONFUSION MATRIX LOGISTICS REGRESSION  TRAINING MODEL  MODEL SCORELogistics RegressionLDATRAINING SET 0.983190,96139TESTING SET 0,978450,956912\\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 53}, page_content='TESTING MODEL \\nOut of total 3342 in testing phase, the model has correctly predicted 3270 with the Accuracy rate of 0.9784(~98%) which is better in terms of Analysis. The Model too predict the variable in Higher Accuracy. The Model deals with Survival not Diagnostics so here the F- score plays an Major Role Linear Discriminant Analysis  Training Model  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 54}, page_content='Testing Model   \\nOut of total 3342 in testing phase, the model has correctly predicted 3198 with the Accuracy rate of 0.9569(~95%) which is better in terms of Analysis  The Model deals with Survival not Diagnostics so here the F- score plays an Major Role.  Classification report  LOGISTICS REGRESSION   \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 55}, page_content='\\nThe F1 score of the Model in Logistics Regression is 0.99  which is fared better in Logistic Regression and also we can check with LDA for getting Most Optimised Model  Linear Discriminant Analysis  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 56}, page_content='The F1 score of the  Model in LDA is 0.98  which is fared better in Logistic Regression  than LDA Model  LOGISTICS REGRESSION  Plot ROC curve and get ROC_AUC score \\nLinear Discriminant Analysis  Plot ROC curve and get ROC_AUC score \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 57}, page_content='On Comparing both the Model, Logistics Regression has Fared better compared to the LDA Model  and also has better AUC score  Henceforth, On Comparing both the model Logistics Regression has fared better than LDA in all the Parameters. One of the Reason is Might be the Reason of Optimisation. Hence after checking the Logistics Regression without optimisation it still had fared better than LDA model. So we can Conclude that the Logistics is the Best and Most optimised Model  Hence the Final output as follow  \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 58}, page_content='2.4) Inference: Based on these predictions, what are the insights and recommendations? (6 marks) Based on the Model,  Insights and Recommendation  â€¢The First is the Speed Category, when ever the speed is low, The survival rate is higher henceforth the speed plays the role in Survival rate .The higher speed the higher is the chance of Death. So recommendation is there should be definitely the speed limit. Below 55 can \\ndefinitely reduce the death  rate \\n'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='â€¢In terms of Weight Category,Lower the weight survival rate is also low in the above data. So recommendation here is to increase the Weight can reduce the Death rate and Increase the Survival rate  â€¢In terms of Airbag, deployment and seat belt.The Survival has become higher in terms of  Vehicle containing the Airbag,Deployment and Seat belt.So the Government is to advice the car maker to provide the Air bag and Seat Belt as compulsory in their Vehicle and it will increase the survival rate  â€¢Interms of Ink severity also whenever the People have essential such as seat belt, Airbag the Injseverity is very low which in turn has produced higher survival rate. The Government can open some First aid centre in Main Highways so that they can Increase the Survival rate of Injured person  â€¢Interms of Year passed, the Vehicle survival rate got Increased which mean that each vehicle should have certain regulation before launching the Vehicle which will definitely increase the Survival rate    '),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 60}, page_content='')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=loader.load()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 0}, page_content='PROJECT ON PREDICTIVE MODELING          (LINEAR REGRESSION, LOGISTICS REGRESSION AND LDA)                                                        M VALLI RAJA SEKAR'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 0}, page_content='PGPDSBA.O.APR22.C'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='Linear Regression  You are a part of an investment firm and your work is to do research about these 759 firms. You are provided with the dataset containing'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='dataset containing the sales and other attributes of these 759 firms. Predict the sales of these firms on the bases of the details given in the dataset so as to help your company in investing'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='in investing consciously. Also, provide them with 5 attributes that are most important. The Main Aim here is to predict the sales using the 759 firms using the Attributes provided for the data and'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='for the data and also to Find the 5 attribute that are most Important for prediction of the data.Here Nine Datasets are Provided   Data Dictionary for Firm_level_data: 1. sales: Sales (in millions of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='(in millions of dollars). 2. capital: Net stock of property, plant, and equipment. 3. patents: Granted patents. 4. randd: R&D stock (in millions of dollars). 5. employment: Employment (in 1000s). 6.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='(in 1000s). 6. sp500: Membership of firms in the S&P 500 index. S&P is a stock market index that measures the stock performance of 500 large companies listed on stock exchanges in the United States'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content=\"the United States 7. tobinq: Tobin's q (also known as q ratio and Kaldor's v) is the ratio between a physical asset's market value and its replacement value. 8. value: Stock market value. 9.\"),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 1}, page_content='market value. 9. institutions: Proportion of stock owned by institutions.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 2}, page_content='1.1) Read the data and do exploratory data analysis. Describe the data briefly. (Check the null values, data types, shape, EDA). Perform Univariate and Bivariate Analysis. (8 marks) The shape of the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 2}, page_content='The shape of the Data contains of total 759 rows and 9 coloumns  The Header of the Data after dropping unnamed will be as follow'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 2}, page_content='The Data contains one object as SP500 which is Categorical in Nature,Whereas the rest of the eight data types are either in Integer or Float in Nature.In which we can clearly Identify as there is'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 2}, page_content='as there is missing Values present in tubing which need to be Identified and Treated'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 3}, page_content='On Identifying the null values, we had observed that the total Null Value Present in data is 21 which can be either treated with Mean or Median'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 3}, page_content='Based on the Assumption, The Missing Value is treated with Mean on rest of the Data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 3}, page_content='Before and Treating the Missing Value with Mean, there is not Much changes in the Data. The Both had Attached above'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 4}, page_content='After Checking the Missing Value, the Next Procedure is to check whether Anomalies or Bad Data is present in the data  In Terms of Bad Data, there is no Bad data is present in the Data sent  For'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 4}, page_content='the Data sent  For Anomalies,'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 4}, page_content='The Randd in above data contains lots of 0 present in the Data. Hence treating those with the value of Mean/ Median.We Might get better Predictions of the data instead of dropping those rows'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 4}, page_content='After Making above changes, we can see that only Means has been slightly changed Whereas the the Rest of the Dataâ€™s are present'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 5}, page_content='Data Visualization Univariate Analysis       Univariate data requires to analyze each variable separately. Uni means one, so in other words the data has only one variable.In Univariate Analysis, we'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 5}, page_content='Analysis, we can use Histogram and Box plot for Numerical type where as countplot for Categorical type  The Histogram and Box Plot Analysis of the eight numeric data are given below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 6}, page_content='The Eight Numerical Data is presented in the form of Histogram and Boxplot and from the above observation we can clearly see that there is lots of Outlier present in the Data and also Histogram are'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 6}, page_content='also Histogram are not much Normally Distributed. Hence Treatment of Outliers and Standardisation is required for the above data.  The Analysis of one Categorical variable of SP500 are as follow'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 7}, page_content='In above data, Only 217 company are part of the SP500 whereas the rest are not part of SP500  Bivariate Analysis            In Bivariate Analysis, there are two variables wherein the analysis is'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 7}, page_content='the analysis is related to cause and the relationship between the two variables.Here the Numeric and Numeric relation are established in the relationship using Pairplot/scatter plot whereas'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 7}, page_content='plot whereas Categorical and Categorical are established with Countplot with Hue'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 8}, page_content='On above analysing the the numerical parameter of the data, we can clearly identify the data Dependent Variable of sales have some kind of correlation ship with all other Independent Variables'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 8}, page_content='Variables present in the data ,while the relationship  can be better identified while finding Correlation ship and also while using the Heat Map for the data.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 8}, page_content='On above Analysing only one Independent Categorical Variable SP500 with only one Dependent Variable of sales.We can Identify that the sales is higher in SP500 member company than comparing with the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 8}, page_content='comparing with the the non member of SP500 The other most fascinating thing is that SP500 member company has higher capital compared to Non SP500 companies'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='Multivariate Analysis  Multivariate descriptive displays or plots are designed to reveal the relationship among several variables simulataneously.. As was the case when examining relationships among'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='relationships among pairs of variables, there are several basic characteristics of the relationship among sets of variables that are of interest. Some of the Interesting Multivariate Analysis are'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='Analysis are pair plot,Heat Map, Facet etc Pairwise Analysis     A pairs plot allows us to see both distribution of single variables and relationships between two variable.The Pairwise plots are'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='Pairwise plots are Mentioned below which determines the Overall relationship between the Dependent Variable and all other Independent variable. From this we Can identify microlevel and we can'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 9}, page_content='and we can establish some kind of Relation'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 11}, page_content='Heat map The heat map is a data visualization technique that shows magnitude of a phenomenon as color in two dimensions. The variation in color may be by hue or intensity, giving obvious visual cues'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 11}, page_content='obvious visual cues to the reader about how the phenomenon is clustered or varies over space.  The Data of correlation and Pictorial representation of Heat map are given below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 12}, page_content='From the Heatmap we can identify the Correlationship of data in better way than establishing  the relationship in Univariate and Bivariate Method  Facetgrid    It is most useful when you have two'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 12}, page_content='when you have two discrete variables, and all combinations of the variables exist in the data.In below data, the higher is the sales, capital and tubing in SP500 compared to non SP500'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 13}, page_content='1.2) Impute null values if present? Do you think scaling is necessary in this case? (8 marks) Impute Null Values- The Data contains one object as SP500 which is Categorical in Nature,Whereas the rest'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 13}, page_content='the rest of the eight data types are either in Integer or Float in Nature.In which we can clearly Identify as there is missing Values present in sp500 which need to be Identified and Treated'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 13}, page_content='On Identifying the null values, we had observed that the total Null Value Present in data is 21 which can be either treated with Mean or Median'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 13}, page_content='Based on the Assumption, The Missing Value of tubinq is  with Mean of the Data   Before and Treating the Missing Value with Mean, there is not Much changes in the Data. The Both had Attached above'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 14}, page_content='After Checking the Missing Value, the Next Procedure is to check whether Anomalies or Bad Data is present in the data  In Terms of Bad Data, there is no Bad data is present in the Data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 14}, page_content='The Randd in above data contains lots of 0 present in the Data. Hence  treating those with the value of Mean/ Median.We Might get better Predictions of the data instead of dropping those rows'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 15}, page_content='After Making above changes, we can see that only Means has been slightly changed Whereas the the Rest of the Dataâ€™s are present are almost same  Hence We had Imputed the Null Values inform of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 15}, page_content='Values inform of Treating With Missing Values, Anamolies and Bad data with either treating Mean of variable  Do you think scaling is necessary in this case'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 16}, page_content='In above we can clearly see that there is difference in the Mean between their Attributes and also each attributes are measured in the different Parameters. Hence Scaling is required in the dataset'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 16}, page_content='in the dataset  On applying the Z score for the data, the Data look'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 16}, page_content='By doing Scaling , The Data got  normalised  and made mean as 0 and standard Deviation as 1. So by scaling all the data are in Normalised Manner'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 17}, page_content='1.3)Encode the data (having string values) for Modelling. Data Split: Split the data into test and train (30:70). Apply Linear regression. Performance Metrics: Check the performance of Predictions on'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 17}, page_content='of Predictions on Train and Test sets using R-square, RMSE'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 17}, page_content='Now we can see that only SP 500  is the only category value  present in the Data. The SP 500 should be converted into either float or Integer by the process of Label Encoding Now info of the Data are'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 17}, page_content='of the Data are as followed'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 18}, page_content='Hence now the whole data contains either float or Integer. Now we can Proceed the Data for Modelling Process  Now we need to split the data into test and Train in 30:70. Which Mean that the data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 18}, page_content='Mean that the data contains 30 %  in test data and the rest 70% in Training data.Before Splitting the data into Test and Train, the first test is segregate Dependent Variable Sales from the Rest  of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 18}, page_content='from the Rest  of the Independent Variable. The Constant should be added to the Variable'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 18}, page_content='Splitting of Data are as follow \\nAs earlier the Test and Train data has  been splited  in the ratio of (30:70) and in the Random state of 5'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 19}, page_content='Applying the Linear regression in the Model using the OLS Method  On the Applying the linear regression using the OLS Method  The Model Summary looks like Below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 19}, page_content='In the Above Model R- Squared and Adj R-Squared Value is 0.928 and 0.927. But lots of attributes Present in the data has P-value >=0.5 hence most of Attributes become less Significant in this case.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 19}, page_content='in this case. So the Multicollinearity using the VIF should be checked and select the last 5 best attributes'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 20}, page_content='In case of VIF for Multicollinearity greater than 4 or 5 can make the curse of Dimensionality. So the Value of VIF should be treated and need to be dropped and then  nessacary actions can be taken'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 20}, page_content='can be taken  The Below Scenario Can be explain the for best scenario of the data Modelling  Scenario 1  Here in scenario 1 our focus will be getting the 5 best attributes based on expert Opinion of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 20}, page_content='expert Opinion of the Data and deducting the nessacary based on our requirement with based on Expert   At first by dropping the Value, the data summary and VIF looks alike'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 21}, page_content='Here after dropping the values, the R squared value dropped to 91.3% and the VIF values is less than 5 but we can have some p-values greater than 0.5 So we can now retain institutions and drop the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 21}, page_content='and drop the tobinq and patents on the basis of Importance of attributes'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 22}, page_content='The model looks like below,'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 22}, page_content='Now comparing to other model, the R and adj R2 has not been changed much compared to other scenario and we can able to get the best attributes here  Paramas of  the Model are as below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 23}, page_content='The equation of the model are as follow,  \\n Some Assumption on Linear Regression- 1).Non Linear on Residual      The Data below are Non linear in Nature'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 23}, page_content='2).Normality    The data are normally Distributed'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 24}, page_content='3)Homoscedastic of Data    The data are Homoscedastic in nature   \\n4)Collinearity    Using VIF, we determined that there is no multicollinearity is present in the data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 25}, page_content='RMSE of the Model    The RMSE of the Model for Training and testing are very low and we can deploy the Model   \\nMean Absolute Error   The MAE of the both training and Testing are also too low'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 25}, page_content='R squared and Adjusted R2 are also 91.3% and 91.2%'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 26}, page_content='The Prediction of the Data   price = 0.028299115503080438 + 0.4777681981583373 * ( -0.168736 ) +  0.14814498002360377 * ( -0.203250 ) +  0.45950021723081313 * ( -0.227855 ) +  0.011256618008760263 * ('),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 26}, page_content='* ( -0.632747 ) +  0.00208124381198248 * ( -1.461405) Price= -0.19729159975652844 The Data has been predicted correctly using trained value on the Tested Value'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 26}, page_content='The Data are scaled in Nature, So the Value is replicating the Negative Value  Hence by  Rescaling we can get the Original Values'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 27}, page_content='Scenario 2  In below  scenario, the RMSE and MSE are higher compared to Scenario 1  The R and Adj R2 reduced drastically to the 0.859 and 0.857. The Scenario can be Rejected We had dropped education'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 27}, page_content='dropped education  from the data instead of Patents.So education cannot be dropped at any scenario'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 28}, page_content='Scenario 3  In below  scenario, the RMSE and MSE are higher compared to Scenario 1  The R and Adj R2 reduced drastically to the 0.859 and 0.857. The Scenario can be Rejected we had dropped all expect'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 28}, page_content='dropped all expect the capital,randd,sp500 and Institutions'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='So from the Over all basis, Scenario 1 better compared other scenarios because of the Higher R2 and adj R2 values in data and lesser error in RMSE  and MAE  So the Scenario 1 is best for the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='1 is best for the Predictions and the 5 attributes that contribute much for the data are as follow'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='Capital ,randd, employment,SP 500 and Institutions are the top 5 attributes  2.4) Inference: Based on these predictions, what are the insights and recommendations? (6 marks) â€¢The capital such as Net'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='capital such as Net Stock of Property,plant and Equipment which plays major in contributing the Sales to the Firm.The Major aspect as an Investment firm is to check the company with the Higher'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='with the Higher Capital which in turn make an higher Sales  â€¢The Second is most important aspect is to check the Employment of the Firm, Whenever the employment got Increased the company make better'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='company make better sales. As an investor is to check the employment in the firm of the Company and to make Better decision while investing in the firm â€¢The R&D plays the role of higher sales. So'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='of higher sales. So while Investing we should check how much the Firm is keen on Investing the R & D. This Parameter makes small edge over other Investment Firm â€¢The Institution  such Proportion of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='such Proportion of Stock owned by Institutions. The Institutions will make huge Investment compared to the retail Investor. So Being an Firm We need to check how much institution contributing for'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='contributing for Stock price will make an added advantage while Investing â€¢In terms of SP 500, the member of Member of SP 500 has higher chance of better sales. So our most important aspect here is'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='aspect here is to check the SP500 company  â€¢Other Attributes such Patents,Dobinq, value etc plays a role but less than what is compared in above in the Model. So we can still concentrate those factor'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 29}, page_content='those factor also'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='Problem 2: Logistic Regression and Linear Discriminant Analysis You are hired by the Government to do an analysis of car crashes. You are provided details of car crashes, among which some people'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content=\"which some people survived and some didn't. You have to help the government in predicting whether a person will survive or not on the basis of the information given in the data set so as to provide\"),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='so as to provide insights that will help the government to make stronger laws for car manufacturers to ensure safety measures. Also, find out the important factors on the basis of which you made your'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='which you made your predictions. 1. dvcat: factor with levels (estimated impact speeds) 1-9km/h, 10-24, 25-39, 40-54, 55+ 2. weight: Observation weights, albeit of uncertain accuracy, designed to'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='designed to account for varying sampling probabilities. (The inverse probability weighting estimator can be used to demonstrate causality when the researcher cannot conduct a controlled experiment'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='experiment but has observed data to model) 3. Survived: factor with levels Survived or not_survived 4. airbag: a factor with levels none or airbag 5. seatbelt: a factor with levels none or belted 6.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='none or belted 6. frontal: a numeric vector; 0 = non-frontal, 1=frontal impact 7. sex: a factor with levels f: Female or m: Male 8. ageOFocc: age of occupant in years 9. yearacc: year of accident 10.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='of accident 10. yearVeh: Year of model of vehicle; a numeric vector 11. abcat: Did one or more (driver or passenger) airbag(s) deploy? This factor has levels deploy, nodeploy and unavail 12. occRole:'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='12. occRole: a factor with levels driver or pass: passenger 13. deploy: a numeric vector: 0 if an airbag was unavailable or did not deploy; 1 if one or more bags deployed. 14. injSeverity: a numeric'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content=\"a numeric vector; 0: none, 1: possible injury, 2: no incapacity, 3: incapacity, 4: killed; 5: unknown, 6: prior death 15. caseid: character, created by pasting together the population's sampling\"),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 30}, page_content='sampling unit, the case number, and the vehicle number. Within each year, use this to uniquely identify the vehicle.'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 31}, page_content='2.1) Data Ingestion: Read the dataset. Do the descriptive statistics and do null value condition check, write an inference on it. Perform Univariate and Bivariate Analysis. Do exploratory data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 31}, page_content='Do exploratory data analysis. (8 marks) The head of the Data are as follow   The shape of the data after removing unnamed:0 as  (11217,15)'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='From the Info of the Data we see clearly that  there is some missing Value in injseverity . The total object is 8,where as float and Integer is totally 7. So we need to check the Missing data,'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='the Missing data, Anomalies and Bad value in the Data and it should be treated'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='For Treating Missing data, Anomalies and Bad value in the Data, info and describe of the should be Verified treating those Value  In above weight is 0 which is kind of Anomalies which should be'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='which should be treated at first, but we can see lot of weight ranging from 0 to 1 which is higher than that of 0 so I had decided not to treat those Anomalies unless and until if there is issue with'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='there is issue with my model  The Next step is to treat the  Missing Value present in the injseverity'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='The total Injseverity missing in Data is 77,This 77 should  be treated for the missing value Here the size of the model is 11217, so we can drop those 77 data which data become 10400 which is still'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 32}, page_content='which is still holding good for prediction  There is no bad value present in the Data and total of two duplicated row and the row got dropped  Hence above the Null check condition is verified'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 33}, page_content='Data Visualisation  Univariate Analysis       Univariate data requires to analyze each variable separately. Uni means one, so in other words the data has only one variable.In Univariate Analysis, we'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 33}, page_content='Analysis, we can use Histogram and Box plot for Numerical type where as countplot for Categorical type  The Histogram and Box Plot Analysis of the eight numeric data are given below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 33}, page_content='The Six Numerical Data is presented in the form of Histogram and Boxplot and from the above observation we can clearly see that there is lots of Outlier present in the Data and also Histogram are not'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 33}, page_content='Histogram are not much Normally Distributed. Hence Treatment of Outliers and Standardisation is required for the above data.  Categorical Variable'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 34}, page_content='In Below are analyse of the Categorical in the data with the value counts. Hence the they are attached below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 38}, page_content='Bivariate Analysis  In Bivariate Analysis, there are two variables wherein the analysis is related to cause and the relationship between the two variables.Here the Numeric and Numeric relation are'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 38}, page_content='relation are established in the relationship using Pairplot/scatter plot whereas Categorical and Categorical are established with Countplot with Hue Scatter Plot'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 38}, page_content='In terms between the InjSeverity and Weight,Higher the Weight the injseverity is low'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 39}, page_content='In between the â€˜yearaccâ€™ and â€˜Injseverityâ€™ ,The Injseverity got reduced when the when the year got  Increased which we can identify that the Introduction of many safety measure got reduced the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 39}, page_content='got reduced the Injseverity'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 39}, page_content='Even  interms of weight also we can see that weight got Increased  as Year Increases'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 39}, page_content='In term of ageOFocc and injseverity we can see that there is kind of no relationship  \\nComparing Independent Variable and Dependent Variable using Bi_variate Analysis'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 40}, page_content='When Injseverity is low, the People got Survived and when it is above 3 there is kind of some causality'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 40}, page_content='When speed is low, there is no causality but when it is above there is kind of Non Survival,When above 55+ there there is less 50% chance for Survival'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 41}, page_content='Whe with the Airbag, the Survival rate is Higher compared to Non Survival without  the Airbag'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 41}, page_content='Initially in year such as 1997, there  is non survival because the reason might be such as Technology, Awarness etc but later in the year on above 2000 there is Kind of Survival Existed but still'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 41}, page_content='Existed but still there is causalities'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='If the Airbag is Unavailed or Nor Deployed then the chance of Causality is higher, When deployed the Survival Rate is higher compared to other scenarios   Multivariate Analysis  Multivariate'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='Multivariate descriptive displays or plots are designed to reveal the relationship among several variables simulataneously.. As was the case when examining relationships among pairs of variables,'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='pairs of variables, there are several basic characteristics of the relationship among sets of variables that are of interest. Some of the Interesting Multivariate Analysis are pair plot,Heat Map,'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='pair plot,Heat Map, Facet etc Pairwise Analysis     A pairs plot allows us to see both distribution of single variables and relationships between two variable.The Pairwise plots are Mentioned below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='are Mentioned below which determines the Overall relationship between the Dependent Variable and all other Independent variable. From this we Can identify microlevel and we can establish some kind of'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 42}, page_content='some kind of Relation which is explained below'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 43}, page_content='Facetgrid- facet_grid() forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 43}, page_content='variables exist in the data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 44}, page_content='Correlation in Data  The Correlation in the following data can be explained in the form of the Pictorial in Heat Map'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 45}, page_content='Correlation measures the relationship between two variables. We mentioned that a function has a purpose to predict a value. We can say also say that a function uses the relationship between two'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 45}, page_content='between two variables for prediction.In above Pictorical Representation will represent more Above about the Correlation'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 46}, page_content='Detecting the Outlier in the Data  Along With those Analysis one of important aspect is the Detecting the Outlier in the Data  We Can see in below that only weight has Outlier while rest of them donâ€™t'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 46}, page_content='rest of them donâ€™t have any kind of Outlier  Its will clearly  visible after doing the Scaling of the Data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='We can Cap the Outlier only in Weight and Year Vech while rest are with the lower values.Hence there is no requirement of detecting outlier in other Attributes unless and until it is essential  2.2)'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='is essential  2.2) Encode the data (having string values) for Modelling. Data Split: Split the data into train and test (70:30). Apply Logistic Regression and LDA (linear discriminant analysis). (8'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='analysis). (8 marks) For Encoding the data  present in the variable we need to first Identify the total categorical variable present in the Data. In the below data we can see there are total of seven'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='are total of seven Category available where as the rest of the data present are in int or Float. The second is to identify how to encode those categorical variable into numerical, All the data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 47}, page_content='All the data mentioned below are ordinal, so we can encode those data point directly instead of using the one hot Encoder'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 48}, page_content='Before Converting the categorical into the Numerical Variable'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 48}, page_content='After Converting those Categorical into the      numerical Variable  \\nBefore Splitting the Data, the most essential thing is to Segregate the Independent and Dependent Variable  in the Data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 49}, page_content='Splitting of data  Logistics Regression  In Logistic Regression, We had Segregated the X and y as X_train,X_test,y_train and y_test with the test size of 0.30 and the random state as 1  Linear'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 49}, page_content='state as 1  Linear Discriminant Analysis  In Linear Discriminant Analysis, We had Segregated the X and y as X_train_1,X_test_1,y_train_1 and y_test_1 with the test size of 0.30 and the random state'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 49}, page_content='the random state as 1  Model Building on the Data  Logistics Regression'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 49}, page_content='In Logistic Regression, We had Build the model using the solver,max_iter,verbose etc to Increase the Accuracy of the Model.We had trained both X and y variable in the Data'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 50}, page_content='In Above We had predicted for both y train_predict and y test_predict and also interns of Probability also'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 51}, page_content='Linear Discriminant Analysis  \\n \\nIn LDA, We had just  trained  the Data in both X and y variable in the Data and not made any complication in Building the LDA Model'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 51}, page_content='In Above We had predicted for both y train_predict_1and y test_predict_1 and also interms of Probability also'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 52}, page_content='2.3) Performance Metrics: Check the performance of Predictions on Train and Test sets using Accuracy, Confusion Matrix, Plot ROC curve and get ROC_AUC score for each model. Compare both the models and'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 52}, page_content='both the models and write inferences, which model is best/optimized. (8 marks) MODEL  ACCURACY SCORE-'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 52}, page_content='In Model  Accuracy Score,The Logistic Regression has fared better than LDA in both Training and Testing.  CONFUSION MATRIX LOGISTICS REGRESSION  TRAINING MODEL  MODEL SCORELogistics'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 52}, page_content='SCORELogistics RegressionLDATRAINING SET 0.983190,96139TESTING SET 0,978450,956912'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 53}, page_content='TESTING MODEL'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 53}, page_content='Out of total 3342 in testing phase, the model has correctly predicted 3270 with the Accuracy rate of 0.9784(~98%) which is better in terms of Analysis. The Model too predict the variable in Higher'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 53}, page_content='variable in Higher Accuracy. The Model deals with Survival not Diagnostics so here the F- score plays an Major Role Linear Discriminant Analysis  Training Model'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 54}, page_content='Testing Model'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 54}, page_content='Out of total 3342 in testing phase, the model has correctly predicted 3198 with the Accuracy rate of 0.9569(~95%) which is better in terms of Analysis  The Model deals with Survival not Diagnostics'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 54}, page_content='not Diagnostics so here the F- score plays an Major Role.  Classification report  LOGISTICS REGRESSION'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 55}, page_content='The F1 score of the Model in Logistics Regression is 0.99  which is fared better in Logistic Regression and also we can check with LDA for getting Most Optimised Model  Linear Discriminant Analysis'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 56}, page_content='The F1 score of the  Model in LDA is 0.98  which is fared better in Logistic Regression  than LDA Model  LOGISTICS REGRESSION  Plot ROC curve and get ROC_AUC score'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 56}, page_content='Linear Discriminant Analysis  Plot ROC curve and get ROC_AUC score'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 57}, page_content='On Comparing both the Model, Logistics Regression has Fared better compared to the LDA Model  and also has better AUC score  Henceforth, On Comparing both the model Logistics Regression has fared'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 57}, page_content='has fared better than LDA in all the Parameters. One of the Reason is Might be the Reason of Optimisation. Hence after checking the Logistics Regression without optimisation it still had fared better'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 57}, page_content='had fared better than LDA model. So we can Conclude that the Logistics is the Best and Most optimised Model  Hence the Final output as follow'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 58}, page_content='2.4) Inference: Based on these predictions, what are the insights and recommendations? (6 marks) Based on the Model,  Insights and Recommendation  â€¢The First is the Speed Category, when ever the speed'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 58}, page_content='when ever the speed is low, The survival rate is higher henceforth the speed plays the role in Survival rate .The higher speed the higher is the chance of Death. So recommendation is there should be'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 58}, page_content='is there should be definitely the speed limit. Below 55 can'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 58}, page_content='definitely reduce the death  rate'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='â€¢In terms of Weight Category,Lower the weight survival rate is also low in the above data. So recommendation here is to increase the Weight can reduce the Death rate and Increase the Survival rate'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='the Survival rate  â€¢In terms of Airbag, deployment and seat belt.The Survival has become higher in terms of  Vehicle containing the Airbag,Deployment and Seat belt.So the Government is to advice the'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='is to advice the car maker to provide the Air bag and Seat Belt as compulsory in their Vehicle and it will increase the survival rate  â€¢Interms of Ink severity also whenever the People have essential'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='have essential such as seat belt, Airbag the Injseverity is very low which in turn has produced higher survival rate. The Government can open some First aid centre in Main Highways so that they can'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='so that they can Increase the Survival rate of Injured person  â€¢Interms of Year passed, the Vehicle survival rate got Increased which mean that each vehicle should have certain regulation before'),\n",
       " Document(metadata={'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf', 'page': 59}, page_content='regulation before launching the Vehicle which will definitely increase the Survival rate')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=200,chunk_overlap=20)\n",
    "splits=text_splitter.split_documents(data)\n",
    "splits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_chroma.vectorstores.Chroma at 0x1223bf640>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding=OllamaEmbeddings()\n",
    "vectordb=Chroma.from_documents(documents=splits,embedding=embedding)\n",
    "vectordb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'page': 11, 'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf'}, page_content='obvious visual cues to the reader about how the phenomenon is clustered or varies over space.  The Data of correlation and Pictorial representation of Heat map are given below'),\n",
       " Document(metadata={'page': 20, 'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf'}, page_content='In case of VIF for Multicollinearity greater than 4 or 5 can make the curse of Dimensionality. So the Value of VIF should be treated and need to be dropped and then  nessacary actions can be taken'),\n",
       " Document(metadata={'page': 15, 'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf'}, page_content='Values inform of Treating With Missing Values, Anamolies and Bad data with either treating Mean of variable  Do you think scaling is necessary in this case'),\n",
       " Document(metadata={'page': 30, 'source': 'Project+-+Predictive+Modeling++%281%29+%281%29 (1).pdf'}, page_content='designed to account for varying sampling probabilities. (The inverse probability weighting estimator can be used to demonstrate causality when the researcher cannot conduct a controlled experiment')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query='Who is context referring here'\n",
    "docs=vectordb.similarity_search(query)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
